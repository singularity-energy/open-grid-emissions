{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate 930 timestamps and interchanges\n",
    "\n",
    "### Generation: \n",
    "\n",
    "Check lagged correlation (-11 to +12 hours) between 930 fossil generation and CEMS fossil generation for each BA over different time bounds: \n",
    "* 2019, 2020, 2021\n",
    "* Daylight savings vs. non- daylight savings\n",
    "\n",
    "Run for both raw, shifted, and shifted + basic and rolling filtered 930 data. When shifts are correct, the best correlation in the shifted data should be at lag=0. The best correlation in the non-shifted data can indicate what shift might be appropriate. Manual inspection is required to actually decide whether and how much to lag by. \n",
    "\n",
    "We run with the rolling-filtered data because in some cases large errors can cause anomalous best correlations in the shifted but not filtered 930 data. \n",
    "\n",
    "Note: for correct timestamps, demand data in non-shifted data will be best correlated at lag=-1 because 930 uses end-of-hour timestamps while CEMS uses start-of-hour. \n",
    "\n",
    "### Interchange: \n",
    "\n",
    "Check lagged correlations between pairs of BAs with shared interchange. If timestamps are consistent, the best correlation should be at lag=0. \n",
    "\n",
    "We also check the sign of the best correlation between paired BAs: if they're not negatively correlated, one of the signs may be incorrect. \n",
    "\n",
    "### Edge cases: \n",
    "\n",
    "In some BAs, the shifted data still shows a best correlation at lag != 0, but inspection of the data doesn't show an obvious fix. In these cases, we do nothing and rely on `gridemissions` to make the data consistent. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import timedelta\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# # Tell python where to look for modules.\n",
    "import sys\n",
    "sys.path.append('../../../open-grid-emissions/src/')\n",
    "\n",
    "import download_data\n",
    "import load_data\n",
    "from column_checks import get_dtypes\n",
    "from filepaths import *\n",
    "import impute_hourly_profiles\n",
    "import data_cleaning\n",
    "import output_data\n",
    "import emissions\n",
    "import validation\n",
    "import gross_to_net_generation\n",
    "import eia930"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data before and after shifts \n",
    "# Note: this is very slow! (~30min) because it's pivoting large files. \n",
    "lraw = []\n",
    "lshift = []\n",
    "\n",
    "for year in [2019, 2020, 2021]:\n",
    "    print(year)\n",
    "\n",
    "    r = eia930.convert_balance_file_to_gridemissions_format(year)\n",
    "\n",
    "    s = eia930.manual_930_adjust(r)\n",
    "    s = eia930.reformat_chalendar(s)\n",
    "    r = eia930.reformat_chalendar(r)\n",
    "\n",
    "    s = s[s.fuel.isin([\"COL\",\"NG\",\"OIL\"])]\n",
    "    s = s.rename(columns={\"UTC Time at End of Hour\": \"datetime_utc\"})\n",
    "    s = s.groupby([\"datetime_utc\",\"BA\"]).sum()[\"generation\"].reset_index()\n",
    "    s = s[s.datetime_utc.dt.year == year] # filter for year\n",
    "\n",
    "    # Filter for fossil fuels, sum by BA\n",
    "    r = r[r.fuel.isin([\"COL\",\"NG\",\"OIL\"])]\n",
    "    r = r.rename(columns={\"UTC Time at End of Hour\": \"datetime_utc\"})\n",
    "    r = r.groupby([\"datetime_utc\",\"BA\"]).sum()[\"generation\"].reset_index()\n",
    "    r = r[r.datetime_utc.dt.year == year] # filter for year\n",
    "    lraw.append(r)\n",
    "    lshift.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.concat(lraw, axis=0)\n",
    "shifted = pd.concat(lshift, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data after shifting and rolling filter \n",
    "\n",
    "all_rolled = []\n",
    "for y in [2019, 2020, 2021]:\n",
    "    rolled_930 = pd.read_csv(f\"../../data/outputs/{y}/eia930/eia930_rolling.csv\", index_col=0, parse_dates=True)\n",
    "    rolled_930 = rolled_930[rolled_930.index.year == y]\n",
    "    all_rolled.append(rolled_930)\n",
    "rolled_930 = eia930.reformat_chalendar(pd.concat(all_rolled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Remove renewables before summing 930 \n",
    "\n",
    "rolled_930 = rolled_930[rolled_930.fuel.isin([\"COL\",\"NG\",\"OIL\"])].groupby([\"datetime_utc\",\"BA\"]).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "# Aggregate by BA during loading to cut down on space\n",
    "cems = pd.DataFrame()\n",
    "for y in [2019, 2020, 2021]: \n",
    "    print(f\"loading {y}\")\n",
    "    file = f\"{data_folder()}/outputs/{y}/cems_cleaned_{y}.csv\"\n",
    "    plant_meta = pd.read_csv(f\"../../data/outputs/{y}/plant_static_attributes_{y}.csv\")\n",
    "    c = pd.read_csv(file, index_col=0, parse_dates=['datetime_utc'])\n",
    "    c = c.rename(columns={\"datetime_utc\":\"datetime_utc\"})\n",
    "    c = c.merge(plant_meta[['plant_id_eia', 'plant_primary_fuel', 'ba_code']], how='left', left_index=True, right_on='plant_id_eia')\n",
    "    # exclude solar power for CEMS, since we're just going to look at COL + OIL + NG in the 930 data\n",
    "    c = c[c[\"plant_primary_fuel\"] != \"SUN\"]\n",
    "    print(\"Aggregating\")\n",
    "    if y == 2021: \n",
    "        c = c.rename(columns={\"gross_generation_mwh\":\"net_generation_mwh\"})\n",
    "    cems_aggregated = c.groupby([\"datetime_utc\",\"ba_code\"]).sum()[\"net_generation_mwh\"].reset_index()\n",
    "    cems = pd.concat([cems, cems_aggregated])\n",
    "\n",
    "cems.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plant_attributes = pd.read_csv(outputs_folder(f\"{year}/plant_static_attributes_{year}.csv\"), dtype=get_dtypes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bas = set(raw.BA.unique())\n",
    "bas.intersection_update(set(cems.ba_code.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"shared BAs: {len(bas)} out of {len(raw.BA.unique())} 930 BAs and {len(cems.ba_code.unique())} CEMS BAs.\")\n",
    "\n",
    "missing_cems = set(raw.BA.unique()).difference(set(cems.ba_code.unique()))\n",
    "missing_930 = set(cems.ba_code.unique()).difference(set(raw.BA.unique()))\n",
    "print(f\"930 BAs missing in CEMS: {missing_cems}\")\n",
    "print(f\"CEMS missing 930: {missing_930}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_cor(cems, df_eia930):\n",
    "    cems = cems.pivot(columns=\"ba_code\", index=\"datetime_utc\", values=\"net_generation_mwh\")\n",
    "    df_eia930 = df_eia930.pivot(columns=\"BA\", index=\"datetime_utc\", values=\"generation\")\n",
    "\n",
    "    bas = set(cems.columns).intersection(set(df_eia930.columns))\n",
    "\n",
    "    correlations = pd.DataFrame(index=bas, columns=range(-12,12), dtype=float)\n",
    "\n",
    "    for ba in correlations.index:\n",
    "        for lag in correlations.columns:\n",
    "            # prepare 930: select BA \n",
    "            #eia = df_eia930[df_eia930.BA==ba][\"generation\"]\n",
    "            # prepare CEMS: select BA\n",
    "            #c = cems[cems.ba_code==ba][\"net_generation_mwh\"]\n",
    "            # calculate \n",
    "            correlations.loc[ba,lag] = cems[ba]\\\n",
    "                .corr(df_eia930[ba].shift(lag))\n",
    "\n",
    "    best = correlations.apply(lambda s: s.index[s.argmax()], axis=1).rename(\"best\")\n",
    "\n",
    "    correlations = pd.concat([best, correlations], axis='columns')\n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cems.drop_duplicates(subset=[\"datetime_utc\",\"ba_code\"], inplace=True)\n",
    "#rolled_930.drop_duplicates(subset=[\"datetime_utc\",\"BA\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate best correlations for shifted (no EBA cleaning) data\n",
    "\n",
    "cems_930_cors = pd.concat([find_best_cor(cems, shifted).best.rename(\"all_years\"),\\\n",
    "    find_best_cor(cems[cems.datetime_utc.dt.year==2019],shifted[shifted.datetime_utc.dt.year==2019]).best.rename(\"2019\"),\n",
    "    find_best_cor(cems[cems.datetime_utc.dt.year==2020],shifted[shifted.datetime_utc.dt.year==2020]).best.rename(\"2020\"),\n",
    "    find_best_cor(cems[cems.datetime_utc.dt.year==2021],shifted[shifted.datetime_utc.dt.year==2021]).best.rename(\"2021\"),\n",
    "    find_best_cor(cems[(cems.datetime_utc.dt.month>=4)&(cems.datetime_utc.dt.month<=9)],\n",
    "        shifted[(shifted.datetime_utc.dt.month>=4)&(shifted.datetime_utc.dt.month<=9)]).best.rename(\"daylight time\"),\n",
    "    find_best_cor(cems[(cems.datetime_utc.dt.month>=11)|(cems.datetime_utc.dt.month<=2)],\n",
    "        shifted[(shifted.datetime_utc.dt.month>=11)|(shifted.datetime_utc.dt.month<=2)]).best.rename(\"standard time\")],\n",
    "    axis='columns')\n",
    "\n",
    "cems_930_cors.to_csv(\"../../data/outputs/2021/cems_SHIFTEDeia930_cor_lags.csv\")\n",
    "#cems_930_cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate best correlations for raw data\n",
    "\n",
    "cems_930_cors = pd.concat([find_best_cor(cems, raw).best.rename(\"all_years\"),\\\n",
    "    find_best_cor(cems[cems.datetime_utc.dt.year==2019],raw[raw.datetime_utc.dt.year==2019]).best.rename(\"2019\"),\n",
    "    find_best_cor(cems[cems.datetime_utc.dt.year==2020],raw[raw.datetime_utc.dt.year==2020]).best.rename(\"2020\"),\n",
    "    find_best_cor(cems[cems.datetime_utc.dt.year==2021],raw[raw.datetime_utc.dt.year==2021]).best.rename(\"2021\"),\n",
    "    find_best_cor(cems[(cems.datetime_utc.dt.month>=4)&(cems.datetime_utc.dt.month<=9)],\n",
    "        raw[(raw.datetime_utc.dt.month>=4)&(raw.datetime_utc.dt.month<=9)]).best.rename(\"daylight time\"),\n",
    "    find_best_cor(cems[(cems.datetime_utc.dt.month>=11)|(cems.datetime_utc.dt.month<=2)],\n",
    "        raw[(raw.datetime_utc.dt.month>=11)|(raw.datetime_utc.dt.month<=2)]).best.rename(\"standard time\")],\n",
    "    axis='columns')\n",
    "\n",
    "cems_930_cors.to_csv(\"../../data/outputs/2021/cems_RAWeia930_cor_lags.csv\")\n",
    "cems_930_cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate correlations using different subsets of 930 data \n",
    "\n",
    "cems_930_cors = pd.concat([find_best_cor(cems, rolled_930).best.rename(\"all_years\"),\\\n",
    "    find_best_cor(cems[cems.datetime_utc.dt.year==2019],rolled_930[rolled_930.datetime_utc.dt.year==2019]).best.rename(\"2019\"),\n",
    "    find_best_cor(cems[cems.datetime_utc.dt.year==2020],rolled_930[rolled_930.datetime_utc.dt.year==2020]).best.rename(\"2020\"),\n",
    "    find_best_cor(cems[cems.datetime_utc.dt.year==2021],rolled_930[rolled_930.datetime_utc.dt.year==2021]).best.rename(\"2021\"),\n",
    "    find_best_cor(cems[(cems.datetime_utc.dt.month>=4)&(cems.datetime_utc.dt.month<=9)],\n",
    "        rolled_930[(rolled_930.datetime_utc.dt.month>=4)&(rolled_930.datetime_utc.dt.month<=9)]).best.rename(\"daylight time\"),\n",
    "    find_best_cor(cems[(cems.datetime_utc.dt.month>=11)|(cems.datetime_utc.dt.month<=2)],\n",
    "        rolled_930[(rolled_930.datetime_utc.dt.month>=11)|(rolled_930.datetime_utc.dt.month<=2)]).best.rename(\"standard time\")],\n",
    "    axis='columns')\n",
    "\n",
    "cems_930_cors.to_csv(\"../../data/outputs/2021/cems_RAWeia930_cor_lags.csv\")\n",
    "cems_930_cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a BA for manual inspection\n",
    "\n",
    "ba = \"SC\"\n",
    "\n",
    "to_plot_930 = shifted[shifted.BA==ba].groupby(\"datetime_utc\").sum()\n",
    "\n",
    "print(f\"correlations for {ba}\")\n",
    "print(cems_930_cors.loc[ba])\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=cems[cems.ba_code==ba].datetime_utc, y=cems[cems.ba_code==ba].net_generation_mwh, name=\"CEMS\"))\n",
    "fig.add_trace(go.Scatter(x=to_plot_930.index, y=to_plot_930.generation, name=\"EIA 930 (after adjustment and rolling cleaning)\"))\n",
    "fig.update_layout(\n",
    "    title=ba,\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Generation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interchange = pd.read_csv(\"../data/eia930/chalendar/EBA_rolling.csv\",index_col=0, parse_dates=True)\n",
    "interchanges = []\n",
    "for year in [2019, 2020, 2021]: \n",
    "    interchange = pd.read_csv(f\"../../data/outputs/{year}/eia930/eia930_raw.csv\",index_col=0, parse_dates=True)\n",
    "    interchange = interchange[interchange.index.year == year] # limit to after gen was reported by fuel type\n",
    "    interchanges.append(interchange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interchange = pd.concat(interchanges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bas930 = {re.split(r\"[-.]\",c)[1] for c in interchange.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a df where columns are interchange data, add best correlation between matching BAs to interchange_cors dict\n",
    "# optionally, write markdown to {file}.md and csvs at {file}_{ba}.csv\n",
    "def interchange_cor(interchange, interchange_cors:dict={}, file=\"\", name:str=\"cors\"):\n",
    "    # Delete file\n",
    "    if file != \"\":\n",
    "        hs = open(file+\".md\",\"w\")\n",
    "        hs.write(\"\\n\\n\")\n",
    "        hs.close() \n",
    "\n",
    "    for ba in bas930:\n",
    "        print(ba, end=\"...\")\n",
    "        other_cols = [c for c in interchange.columns \\\n",
    "            if re.split(r\"[-.]\",c)[1]==ba \\\n",
    "                and re.split(r\"[-.]\",c)[2]!=\"ALL\"]\n",
    "        other_bas = [re.split(r\"[-.]\",c)[2] for c in other_cols]\n",
    "        #print(f\"{ba} connects to {other_bas}\")\n",
    "\n",
    "        out = pd.DataFrame(index=other_bas, columns=range(-12,12), dtype=float)\n",
    "        for o_ba in out.index:\n",
    "            this_way = f\"EBA.{o_ba}-{ba}.ID.H\"\n",
    "            other_way = f\"EBA.{ba}-{o_ba}.ID.H\"\n",
    "            if other_way not in interchange.columns or this_way not in interchange.columns: \n",
    "                continue\n",
    "            for lag in out.columns:\n",
    "                out.loc[o_ba,lag] = abs(interchange[this_way]\\\n",
    "                    .corr(-1*interchange[other_way].shift(lag)))\n",
    "        \n",
    "        # where is correlation the best?\n",
    "        out = pd.concat([out, out.apply(lambda s: s.index[s.argmax()], axis=1).rename(\"best\")], axis='columns')\n",
    "\n",
    "        if file != \"\":\n",
    "            # add new lines for proper markdown syntax\n",
    "            hs = open(file+\".md\",\"a\")\n",
    "            hs.write(f\"\\n\\n# {ba}\\n\\n\")\n",
    "            hs.close() \n",
    "\n",
    "            out.to_markdown(file+\".md\",mode=\"a\")\n",
    "\n",
    "            out.to_csv(f\"{file}_{ba}\"+\".csv\")\n",
    "\n",
    "        interchange_cors[ba] = pd.concat([interchange_cors.get(ba, pd.DataFrame()), out.best.rename(name)], axis='columns')\n",
    "\n",
    "    return interchange_cors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cors = interchange_cor(interchange, interchange_cors={}, name=\"all_years\")\n",
    "int_cors = interchange_cor(interchange[\"2019-01-01T00:00\":\"2019-12-30T00:00\"], int_cors, name=\"2019\")\n",
    "int_cors = interchange_cor(interchange[\"2020-01-01T00:00\":\"2020-12-30T00:00\"], int_cors, name=\"2020\")\n",
    "int_cors = interchange_cor(interchange[\"2021-01-01T00:00\":\"2021-12-30T00:00\"], int_cors, name=\"2021\")\n",
    "int_cors = interchange_cor(interchange[(interchange.index.month >= 4)&(interchange.index.month <=9)], int_cors, name=\"daylight savings\")\n",
    "int_cors = interchange_cor(interchange[(interchange.index.month >= 11)|(interchange.index.month <=2)], int_cors, name=\"standard time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect interchange correlations\n",
    "\n",
    "int_cors[\"PJM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to md file because that's an easy way to manually scan through BAs and look for anomalies\n",
    "\n",
    "file = \"../../data/outputs/2021/interchange_corr_summary_adjusted.md\"\n",
    "hs = open(file,\"w\")\n",
    "hs.write(\"\\n\\n\")\n",
    "hs.close() \n",
    "\n",
    "for (ba,out) in int_cors.items():\n",
    "\n",
    "    # add new lines for proper markdown syntax\n",
    "            hs = open(file,\"a\")\n",
    "            hs.write(f\"\\n\\n# {ba}\\n\\n\")\n",
    "            hs.close() \n",
    "\n",
    "            out.to_markdown(file,mode=\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot interchange for BA of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba1 = \"AEC\"\n",
    "ba2 = \"MISO\"\n",
    "\n",
    "fig = px.line(interchange[f\"EBA.{ba1}-{ba2}.ID.H\"])\n",
    "fig.add_trace(go.Scatter(x=interchange.index, y=interchange[f\"EBA.{ba2}-{ba1}.ID.H\"], name=f\"EBA.{ba2}-{ba1}.ID.H\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba = \"PJM\"\n",
    "\n",
    "# find cols of mappings in both directions \n",
    "other_cols = [c for c in interchange.columns \\\n",
    "    if re.split(r\"[-.]\",c)[1]==ba \\\n",
    "        and re.split(r\"[-.]\",c)[2]!=\"ALL\"]\n",
    "other_bas = [re.split(r\"[-.]\",c)[2] for c in other_cols]\n",
    "\n",
    "these_cols = [f\"EBA.{o_ba}-{ba}.ID.H\" for o_ba in other_bas]\n",
    "\n",
    "# make long version with just cols of interest, adding BA column and to/from column\n",
    "toplot = pd.DataFrame()\n",
    "for i in range(len(other_bas)): \n",
    "    to_add = (interchange[other_cols[i]]).rename(\"interchange\").to_frame()\n",
    "    to_add[\"source\"] = ba\n",
    "    to_add[\"BA\"] = other_bas[i]\n",
    "\n",
    "    to_add_2 = (interchange[these_cols[i]]*(-1)).rename(\"interchange\").to_frame()\n",
    "    to_add_2[\"source\"] = \"other BA\"\n",
    "    to_add_2[\"BA\"] = other_bas[i]\n",
    "\n",
    "    toplot = pd.concat([toplot, to_add, to_add_2], axis='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(toplot, x=toplot.index, y=\"interchange\", facet_col=\"BA\", facet_col_wrap=2, color=\"source\")\n",
    "fig.update_layout(\n",
    "    title=f\"Interchange from {ba}\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Interchange\",\n",
    "    legend_title=\"Source for<br>interchange data\"\n",
    ")\n",
    "fig.for_each_annotation(lambda a: a.update(text=\"Other \"+a.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first=\"PJM\"\n",
    "second=\"MISO\"\n",
    "\n",
    "fig = px.line(interchange, x=interchange.index, y=[f\"EBA.{first}-{second}.ID.H\",f\"EBA.{second}-{first}.ID.H\", f\"EBA.{first}-ALL.TI.H\"])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"{first}/{second} interchange\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Interchange\",\n",
    "    legend_title=\"Series\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba = \"CFE\"\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=interchange.index, \n",
    "    y=interchange[f\"EBA.{ba}-ALL.D.H\"]-interchange[f\"EBA.{ba}-ALL.NG.H\"]))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"{ba} demand - generation\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Demand - generation\",\n",
    "    legend_title=\"Series\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign issues across interchange data\n",
    "\n",
    "Most interchanges should be negatively correlated with the interchange coming the other way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a df where columns are interchange data, add best correlation between matching BAs to interchange_cors dict\n",
    "# optionally, write markdown to {file}.md and csvs at {file}_{ba}.csv\n",
    "def interchange_sign(interchange, i_sign:dict={}, file=\"\", name:str=\"cors\"):\n",
    "    for ba in bas930:\n",
    "        print(ba, end=\"...\")\n",
    "        other_cols = [c for c in interchange.columns \\\n",
    "            if re.split(r\"[-.]\",c)[1]==ba \\\n",
    "                and re.split(r\"[-.]\",c)[2]!=\"ALL\"]\n",
    "        other_bas = [re.split(r\"[-.]\",c)[2] for c in other_cols]\n",
    "        #print(f\"{ba} connects to {other_bas}\")\n",
    "\n",
    "        out = pd.DataFrame(index=other_bas, columns=range(-12,12), dtype=float)\n",
    "        for o_ba in out.index:\n",
    "            this_way = f\"EBA.{o_ba}-{ba}.ID.H\"\n",
    "            other_way = f\"EBA.{ba}-{o_ba}.ID.H\"\n",
    "            if other_way not in interchange or this_way not in interchange: \n",
    "                continue \n",
    "            for lag in out.columns:\n",
    "                out.loc[o_ba,lag] = interchange[this_way]\\\n",
    "                    .corr(-1*interchange[other_way].shift(lag))\n",
    "        \n",
    "        # where is correlation the best?\n",
    "        out = out.apply(lambda s: s.iloc[abs(s).argmax()], axis=1)\n",
    "\n",
    "        i_sign[ba] = pd.concat([i_sign.get(ba, pd.DataFrame()), out.rename(name)], axis='columns')\n",
    "\n",
    "    return i_sign\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_sign = interchange_sign(interchange, {}, name=\"all_years\")\n",
    "int_sign = interchange_sign(interchange[\"2019-01-01T00:00\":\"2019-12-30T00:00\"], int_sign, name=\"2019\")\n",
    "int_sign = interchange_sign(interchange[\"2020-01-01T00:00\":\"2020-12-30T00:00\"], int_sign, name=\"2020\")\n",
    "int_sign = interchange_sign(interchange[\"2020-01-01T00:00\":\"2020-12-30T00:00\"], int_sign, name=\"2021\")\n",
    "int_sign = interchange_sign(interchange[(interchange.index.month >= 4)&(interchange.index.month <=9)], int_sign, name=\"daylight savings\")\n",
    "int_sign = interchange_sign(interchange[(interchange.index.month >= 11)|(interchange.index.month <=2)], int_sign, name=\"standard time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = f\"{outputs_folder('2021')}/interchange_cors_sign.md\"\n",
    "hs = open(file,\"w\")\n",
    "hs.write(\"\\n\\n\")\n",
    "hs.close() \n",
    "\n",
    "for (ba,out) in int_sign.items():\n",
    "\n",
    "    # add new lines for proper markdown syntax\n",
    "            hs = open(file,\"a\")\n",
    "            hs.write(f\"\\n\\n# {ba}\\n\\n\")\n",
    "            hs.close() \n",
    "\n",
    "            out.to_markdown(file,mode=\"a\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('open_grid_emissions')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3035f9e4886fbbd36e5472dea39a16278dd2b875736b64746b1db5f69a1d6c82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
