{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate 930 timestamps and interchanges\n",
    "\n",
    "### Generation: \n",
    "\n",
    "Check lagged correlation (-11 to +12 hours) between 930 fossil generation and CEMS fossil generation for each BA over different time bounds: \n",
    "* 2018, 2019, 2020, 2021, 2022, 2023, 2024\n",
    "* Daylight savings vs. non- daylight savings\n",
    "\n",
    "Run for both raw, shifted, and shifted + basic and rolling filtered 930 data. When shifts are correct, the best correlation in the shifted data should be at lag=0. The best correlation in the non-shifted data can indicate what shift might be appropriate. Manual inspection is required to actually decide whether and how much to lag by. \n",
    "\n",
    "We run with the rolling-filtered data because in some cases large errors can cause anomalous best correlations in the shifted but not filtered 930 data. \n",
    "\n",
    "Note: for correct timestamps, demand data in non-shifted data will be best correlated at lag=-1 because 930 uses end-of-hour timestamps while CEMS uses start-of-hour. \n",
    "\n",
    "### Interchange: \n",
    "\n",
    "Check lagged correlations between pairs of BAs with shared interchange. If timestamps are consistent, the best correlation should be at lag=0. \n",
    "\n",
    "We also check the sign of the best correlation between paired BAs: if they're not negatively correlated, one of the signs may be incorrect. \n",
    "\n",
    "### Edge cases: \n",
    "\n",
    "In some BAs, the shifted data still shows a best correlation at lag != 0, but inspection of the data doesn't show an obvious fix. In these cases, we do nothing and rely on `gridemissions` to make the data consistent. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from oge.column_checks import get_dtypes\n",
    "from oge.filepaths import *\n",
    "import oge.eia930 as eia930"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data before and after timestamp shifts\n",
    "# Note: this is very slow! (~2min per year) because it's pivoting large files.\n",
    "# Only consider thermal fuels (coal, natural gas, petroleum) in EIA-930 data for\n",
    "# comparison with CEMS.\n",
    "lraw = []\n",
    "lshift = []\n",
    "\n",
    "for year in [2019, 2020, 2021, 2022, 2023, 2024]:\n",
    "    print(year)\n",
    "\n",
    "    r = eia930.convert_balance_data_to_gridemissions_format(year)\n",
    "\n",
    "    s = eia930.manual_930_adjust(r)\n",
    "    s = eia930.reformat_chalendar(s)\n",
    "    r = eia930.reformat_chalendar(r)\n",
    "\n",
    "    s = s[s[\"fuel\"].isin([\"COL\", \"NG\", \"OIL\"])]\n",
    "    s = s.rename(columns={\"UTC Time at End of Hour\": \"datetime_utc\"})\n",
    "    s = s.groupby([\"datetime_utc\", \"BA\"]).sum()[\"generation\"].reset_index()\n",
    "    s = s[s[\"datetime_utc\"].dt.year == year]  # filter for year\n",
    "\n",
    "    # Filter for fossil fuels, sum by BA\n",
    "    r = r[r[\"fuel\"].isin([\"COL\", \"NG\", \"OIL\"])]\n",
    "    r = r.rename(columns={\"UTC Time at End of Hour\": \"datetime_utc\"})\n",
    "    r = r.groupby([\"datetime_utc\", \"BA\"]).sum()[\"generation\"].reset_index()\n",
    "    r = r[r[\"datetime_utc\"].dt.year == year]  # filter for year\n",
    "\n",
    "    lraw.append(r)\n",
    "    lshift.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.concat(lraw, axis=0)\n",
    "shifted = pd.concat(lshift, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data after shifting and rolling filter\n",
    "all_rolled = []\n",
    "for y in [2019, 2020, 2021, 2022, 2023, 2024]:\n",
    "    rolled_930 = pd.read_csv(\n",
    "        f\"{data_folder()}/outputs/{y}/eia930/eia930_rolling.csv\",\n",
    "        index_col=0,\n",
    "        parse_dates=True,\n",
    "    )\n",
    "    rolled_930 = rolled_930[rolled_930.index.year == y]\n",
    "    all_rolled.append(rolled_930)\n",
    "rolled_930 = eia930.reformat_chalendar(pd.concat(all_rolled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove renewables before summing EIA-930\n",
    "rolled_930 = (\n",
    "    rolled_930[rolled_930[\"fuel\"].isin([\"COL\", \"NG\", \"OIL\"])]\n",
    "    .groupby([\"datetime_utc\", \"BA\"])\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "# Aggregate by BA during loading to cut down on space\n",
    "cems = pd.DataFrame()\n",
    "for y in [2019, 2020, 2021, 2022, 2023, 2024]:\n",
    "    print(f\"loading {y}\")\n",
    "    file = f\"{data_folder()}/outputs/{y}/cems_cleaned_{y}.csv.zip\"\n",
    "    plant_meta = pd.read_csv(\n",
    "        f\"{data_folder()}/outputs/{y}/plant_static_attributes_{y}.csv.zip\"\n",
    "    )\n",
    "    c = pd.read_csv(file, index_col=0, parse_dates=[\"datetime_utc\"], low_memory=False)\n",
    "    c = c.merge(\n",
    "        plant_meta[[\"plant_id_eia\", \"plant_primary_fuel\", \"ba_code\"]],\n",
    "        how=\"left\",\n",
    "        left_index=True,\n",
    "        right_on=\"plant_id_eia\",\n",
    "    )\n",
    "    # Exclude solar\n",
    "    c = c[c[\"plant_primary_fuel\"] != \"SUN\"]\n",
    "    c = c[[\"datetime_utc\", \"ba_code\", \"gross_generation_mwh\"]]\n",
    "\n",
    "    print(\"Aggregating\")\n",
    "    if y in [2019, 2020, 2021, 2022, 2023, 2024]:\n",
    "        c = c.rename(columns={\"gross_generation_mwh\": \"net_generation_mwh\"})\n",
    "    cems_aggregated = (\n",
    "        c.groupby([\"datetime_utc\", \"ba_code\"]).sum()[\"net_generation_mwh\"].reset_index()\n",
    "    )\n",
    "    cems = pd.concat([cems, cems_aggregated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2024\n",
    "plant_attributes = pd.read_csv(\n",
    "    f\"{data_folder()}/outputs/{year}/plant_static_attributes_{year}.csv.zip\",\n",
    "    dtype=get_dtypes(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bas = set(raw[\"BA\"].unique())\n",
    "bas.intersection_update(set(cems[\"ba_code\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"shared BAs: {len(bas)} out of {len(raw['BA'].unique())} 930 BAs and \"\n",
    "    f\"{len(cems['ba_code'].unique())} CEMS BAs.\"\n",
    ")\n",
    "\n",
    "missing_cems = set(raw[\"BA\"].unique()).difference(set(cems[\"ba_code\"].unique()))\n",
    "missing_930 = set(cems[\"ba_code\"].unique()).difference(set(raw[\"BA\"].unique()))\n",
    "print(f\"930 BAs missing in CEMS: {missing_cems}\")\n",
    "print(f\"CEMS missing 930: {missing_930}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_cor(cems, df_eia930):\n",
    "    cems = cems.pivot(\n",
    "        columns=\"ba_code\", index=\"datetime_utc\", values=\"net_generation_mwh\"\n",
    "    )\n",
    "    df_eia930 = df_eia930.pivot(columns=\"BA\", index=\"datetime_utc\", values=\"generation\")\n",
    "\n",
    "    bas = set(cems.columns).intersection(set(df_eia930.columns))\n",
    "\n",
    "    # Create multi-level columns: month names x (lags + best)\n",
    "    month_names = [\n",
    "        \"January\",\n",
    "        \"February\",\n",
    "        \"March\",\n",
    "        \"April\",\n",
    "        \"May\",\n",
    "        \"June\",\n",
    "        \"July\",\n",
    "        \"August\",\n",
    "        \"September\",\n",
    "        \"October\",\n",
    "        \"November\",\n",
    "        \"December\",\n",
    "    ]\n",
    "    lags = list(range(-12, 13))  # -12 to 12 inclusive\n",
    "    lag_cols = lags + [\"best\"]\n",
    "    multi_cols = pd.MultiIndex.from_product(\n",
    "        [month_names, lag_cols], names=[\"month\", \"lag\"]\n",
    "    )\n",
    "\n",
    "    correlations = pd.DataFrame(index=list(bas), columns=multi_cols, dtype=float)\n",
    "\n",
    "    for ba in correlations.index:\n",
    "        for month_idx, month_name in enumerate(month_names, 1):\n",
    "            # Get data for this specific month\n",
    "            cems_month = cems[ba][cems.index.month == month_idx]\n",
    "            eia930_month = df_eia930[ba][df_eia930.index.month == month_idx]\n",
    "\n",
    "            # Get index where both have non-zero and not-missing values\n",
    "            idx_for_corr = (\n",
    "                cems_month.notna()\n",
    "                & eia930_month.notna()\n",
    "                & (cems_month != 0)\n",
    "                & (eia930_month != 0)\n",
    "            )\n",
    "\n",
    "            # Check if we have enough data points for meaningful correlation\n",
    "            if cems_month[idx_for_corr].empty or eia930_month[idx_for_corr].empty:\n",
    "                continue\n",
    "\n",
    "            valid_cems = cems_month[idx_for_corr]\n",
    "            valid_eia930 = eia930_month[idx_for_corr]\n",
    "\n",
    "            # Need at least 3 data points for correlation and non-zero variance\n",
    "            if len(valid_cems) < 3 or valid_cems.std() == 0 or valid_eia930.std() == 0:\n",
    "                continue\n",
    "\n",
    "            # Calculate correlations for each lag\n",
    "            month_correlations = {}\n",
    "            for lag in lags:\n",
    "                try:\n",
    "                    shifted_eia930 = valid_eia930.shift(lag).dropna()\n",
    "                    # Align indices after shifting\n",
    "                    common_idx = valid_cems.index.intersection(shifted_eia930.index)\n",
    "\n",
    "                    if len(common_idx) < 3:\n",
    "                        continue\n",
    "\n",
    "                    cems_aligned = valid_cems.loc[common_idx]\n",
    "                    eia930_aligned = shifted_eia930.loc[common_idx]\n",
    "\n",
    "                    # Check for zero variance after alignment\n",
    "                    if cems_aligned.std() == 0 or eia930_aligned.std() == 0:\n",
    "                        continue\n",
    "\n",
    "                    corr_val = cems_aligned.corr(eia930_aligned)\n",
    "\n",
    "                    # Only store if correlation is valid\n",
    "                    if pd.notna(corr_val) and not np.isinf(corr_val):\n",
    "                        correlations.loc[ba, (month_name, lag)] = corr_val\n",
    "                        month_correlations[lag] = corr_val\n",
    "\n",
    "                except (ValueError, ZeroDivisionError, RuntimeWarning):\n",
    "                    # Skip this lag if calculation fails\n",
    "                    print(f\"Skipping {lag}h lag for {ba} in {month_name}\")\n",
    "                    continue\n",
    "\n",
    "            # Find best lag for this month\n",
    "            if month_correlations:\n",
    "                # Find lag with maximum correlation (handling NaN values)\n",
    "                valid_correlations = {\n",
    "                    k: v for k, v in month_correlations.items() if pd.notna(v)\n",
    "                }\n",
    "                if valid_correlations:\n",
    "                    best_lag = max(\n",
    "                        valid_correlations.keys(), key=lambda x: valid_correlations[x]\n",
    "                    )\n",
    "                    best_corr = valid_correlations[best_lag]\n",
    "                    correlations.loc[ba, (month_name, \"best\")] = (\n",
    "                        best_lag if best_corr > 0.5 else np.nan\n",
    "                    )\n",
    "\n",
    "    correlations.dropna(how=\"all\", inplace=True)\n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cems.drop_duplicates(subset=[\"datetime_utc\", \"ba_code\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best lags across all months for summary\n",
    "year_dfs = {}\n",
    "for year in [2019, 2020, 2021, 2022, 2023, 2024]:\n",
    "    print(f\"### Year: {year} ###\")\n",
    "    year_result = find_best_cor(\n",
    "        cems[cems[\"datetime_utc\"].dt.year == year],\n",
    "        shifted[shifted[\"datetime_utc\"].dt.year == year],\n",
    "    ).xs(\"best\", level=\"lag\", axis=1)\n",
    "    year_dfs[str(year)] = year_result\n",
    "\n",
    "cems_930_cors = pd.concat(year_dfs, axis=\"columns\")\n",
    "\n",
    "# Remove BAs where all values are either zero or NA\n",
    "mask = ~((cems_930_cors == 0.0) | cems_930_cors.isna()).all(axis=1)\n",
    "filtered_cors = cems_930_cors.loc[mask]\n",
    "filtered_cors.to_csv(f\"{data_folder()}/outputs/2024/cems_shifted_eia930_lags.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best lags across all months for summary\n",
    "year_dfs = {}\n",
    "for year in [2019, 2020, 2021, 2022, 2023, 2024]:\n",
    "    print(f\"### Year: {year} ###\")\n",
    "    year_result = find_best_cor(\n",
    "        cems[cems[\"datetime_utc\"].dt.year == year],\n",
    "        raw[raw[\"datetime_utc\"].dt.year == year],\n",
    "    ).xs(\"best\", level=\"lag\", axis=1)\n",
    "    year_dfs[str(year)] = year_result\n",
    "\n",
    "cems_930_cors = pd.concat(year_dfs, axis=\"columns\")\n",
    "\n",
    "# Remove BAs where all values are either -1 or NA\n",
    "mask = ~((cems_930_cors == -1.0) | cems_930_cors.isna()).all(axis=1)\n",
    "filtered_cors = cems_930_cors.loc[mask]\n",
    "filtered_cors.to_csv(f\"{data_folder()}/outputs/2024/cems_raw_eia930_lags.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best lags across all months for summary\n",
    "year_dfs = {}\n",
    "for year in [2019, 2020, 2021, 2022, 2023, 2024]:\n",
    "    print(f\"### Year: {year} ###\")\n",
    "    year_result = find_best_cor(\n",
    "        cems[cems[\"datetime_utc\"].dt.year == year],\n",
    "        rolled_930[rolled_930[\"datetime_utc\"].dt.year == year],\n",
    "    ).xs(\"best\", level=\"lag\", axis=1)\n",
    "    year_dfs[str(year)] = year_result\n",
    "\n",
    "cems_930_cors = pd.concat(year_dfs, axis=\"columns\")\n",
    "\n",
    "cems_930_cors.to_csv(f\"{data_folder()}/outputs/2024/cems_rolled_eia930_lags.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a BA for manual inspection\n",
    "ba = \"AVA\"\n",
    "\n",
    "to_plot_930 = shifted[shifted[\"BA\"] == ba].groupby(\"datetime_utc\").sum()\n",
    "\n",
    "print(f\"correlations for {ba}\")\n",
    "print(cems_930_cors.loc[ba])\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=cems[cems[\"ba_code\"] == ba][\"datetime_utc\"],\n",
    "        y=cems[cems[\"ba_code\"] == ba][\"net_generation_mwh\"],\n",
    "        name=\"CEMS\",\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=to_plot_930.index,\n",
    "        y=to_plot_930[\"generation\"],\n",
    "        name=\"Adjusted EIA-930\",\n",
    "    )\n",
    ")\n",
    "fig.update_layout(title=ba, xaxis_title=\"Date\", yaxis_title=\"Generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interchanges = []\n",
    "for year in [2019, 2020, 2021, 2022, 2023, 2024]:\n",
    "    interchange = pd.read_csv(\n",
    "        f\"{data_folder()}/outputs/{year}/eia930/eia930_raw.csv\",\n",
    "        index_col=0,\n",
    "        parse_dates=True,\n",
    "    )\n",
    "    interchange = interchange[\n",
    "        interchange.index.year == year\n",
    "    ]  # limit to after gen was reported by fuel type\n",
    "    interchanges.append(interchange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interchange = pd.concat(interchanges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bas930 = {re.split(r\"[-.]\", c)[1] for c in interchange.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a df where columns are interchange data, add best correlation between matching\n",
    "# BAs to interchange_cors dict, optionally, write markdown to {file}.md and csvs at\n",
    "# {file}_{ba}.csv\n",
    "def interchange_cor(\n",
    "    interchange, interchange_cors: dict = {}, file=\"\", name: str = \"cors\"\n",
    "):\n",
    "    # Delete file\n",
    "    if file != \"\":\n",
    "        hs = open(file + \".md\", \"w\")\n",
    "        hs.write(\"\\n\\n\")\n",
    "        hs.close()\n",
    "\n",
    "    for ba in bas930:\n",
    "        print(ba, end=\"...\")\n",
    "        other_cols = [\n",
    "            c\n",
    "            for c in interchange.columns\n",
    "            if re.split(r\"[-.]\", c)[1] == ba and re.split(r\"[-.]\", c)[2] != \"ALL\"\n",
    "        ]\n",
    "        other_bas = [re.split(r\"[-.]\", c)[2] for c in other_cols]\n",
    "\n",
    "        out = pd.DataFrame(index=other_bas, columns=range(-12, 12), dtype=float)\n",
    "        for o_ba in out.index:\n",
    "            this_way = f\"EBA.{o_ba}-{ba}.ID.H\"\n",
    "            other_way = f\"EBA.{ba}-{o_ba}.ID.H\"\n",
    "            if (\n",
    "                other_way not in interchange.columns\n",
    "                or this_way not in interchange.columns\n",
    "            ):\n",
    "                continue\n",
    "            idx_for_corr = (\n",
    "                interchange[this_way].notna()\n",
    "                & interchange[other_way].notna()\n",
    "                & (interchange[this_way] != 0)\n",
    "                & (interchange[other_way] != 0)\n",
    "            )\n",
    "            if (\n",
    "                interchange[this_way][idx_for_corr].empty\n",
    "                or interchange[other_way][idx_for_corr].empty\n",
    "            ):\n",
    "                continue\n",
    "            else:\n",
    "                for lag in out.columns:\n",
    "                    out.loc[o_ba, lag] = abs(\n",
    "                        interchange[this_way].corr(\n",
    "                            -1 * interchange[other_way].shift(lag)\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        out.dropna(how=\"all\", inplace=True)\n",
    "        best = out.idxmax(skipna=True, axis=1).rename(\"best\")\n",
    "\n",
    "        out = pd.concat([best, out], axis=\"columns\")\n",
    "\n",
    "        if file != \"\":\n",
    "            # add new lines for proper markdown syntax\n",
    "            hs = open(file + \".md\", \"a\")\n",
    "            hs.write(f\"\\n\\n# {ba}\\n\\n\")\n",
    "            hs.close()\n",
    "\n",
    "            out.to_markdown(file + \".md\", mode=\"a\")\n",
    "\n",
    "            out.to_csv(f\"{file}_{ba}\" + \".csv\")\n",
    "\n",
    "        interchange_cors[ba] = pd.concat(\n",
    "            [interchange_cors.get(ba, pd.DataFrame()), out.best.rename(name)],\n",
    "            axis=\"columns\",\n",
    "        )\n",
    "\n",
    "    return interchange_cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cors = {}\n",
    "for year in [2019, 2020, 2021, 2022, 2023, 2024]:\n",
    "    int_cors = interchange_cor(\n",
    "        interchange[f\"{year}-01-01T00:00\" : f\"{year}-12-30T00:00\"],\n",
    "        int_cors,\n",
    "        name=str(year),\n",
    "    )\n",
    "\n",
    "int_cors = interchange_cor(interchange, int_cors, name=\"all_years\")\n",
    "int_cors = interchange_cor(\n",
    "    interchange[(interchange.index.month >= 4) & (interchange.index.month <= 9)],\n",
    "    int_cors,\n",
    "    name=\"daylight savings\",\n",
    ")\n",
    "int_cors = interchange_cor(\n",
    "    interchange[(interchange.index.month >= 11) | (interchange.index.month <= 2)],\n",
    "    int_cors,\n",
    "    name=\"standard time\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect interchange correlations\n",
    "int_cors[\"PJM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to md file because that's an easy way to manually scan through BAs and look for anomalies\n",
    "file = f\"{data_folder()}/outputs/2024/interchange_corr_summary_adjusted.md\"\n",
    "hs = open(file, \"w\")\n",
    "hs.write(\"\\n\\n\")\n",
    "hs.close()\n",
    "\n",
    "for ba, out in int_cors.items():\n",
    "    # add new lines for proper markdown syntax\n",
    "    hs = open(file, \"a\")\n",
    "    hs.write(f\"\\n\\n# {ba}\\n\\n\")\n",
    "    hs.close()\n",
    "\n",
    "    out.to_markdown(file, mode=\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot interchange for BA of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba1 = \"IID\"\n",
    "ba2 = \"CISO\"\n",
    "\n",
    "fig = px.line(interchange[f\"EBA.{ba1}-{ba2}.ID.H\"])\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=interchange.index,\n",
    "        y=interchange[f\"EBA.{ba2}-{ba1}.ID.H\"],\n",
    "        name=f\"EBA.{ba2}-{ba1}.ID.H\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba = \"PJM\"\n",
    "\n",
    "# find cols of mappings in both directions\n",
    "other_cols = [\n",
    "    c\n",
    "    for c in interchange.columns\n",
    "    if re.split(r\"[-.]\", c)[1] == ba and re.split(r\"[-.]\", c)[2] != \"ALL\"\n",
    "]\n",
    "other_bas = [re.split(r\"[-.]\", c)[2] for c in other_cols]\n",
    "\n",
    "these_cols = [f\"EBA.{o_ba}-{ba}.ID.H\" for o_ba in other_bas]\n",
    "\n",
    "# make long version with just cols of interest, adding BA column and to/from column\n",
    "toplot = pd.DataFrame()\n",
    "for i in range(len(other_bas)):\n",
    "    to_add = (interchange[other_cols[i]]).rename(\"interchange\").to_frame()\n",
    "    to_add[\"source\"] = ba\n",
    "    to_add[\"BA\"] = other_bas[i]\n",
    "\n",
    "    to_add_2 = (interchange[these_cols[i]] * (-1)).rename(\"interchange\").to_frame()\n",
    "    to_add_2[\"source\"] = \"other BA\"\n",
    "    to_add_2[\"BA\"] = other_bas[i]\n",
    "\n",
    "    toplot = pd.concat([toplot, to_add, to_add_2], axis=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    toplot,\n",
    "    x=toplot.index,\n",
    "    y=\"interchange\",\n",
    "    facet_col=\"BA\",\n",
    "    facet_col_wrap=2,\n",
    "    color=\"source\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=f\"Interchange from {ba}\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Interchange\",\n",
    "    legend_title=\"Source for<br>interchange data\",\n",
    ")\n",
    "fig.for_each_annotation(lambda a: a.update(text=\"Other \" + a.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = \"PJM\"\n",
    "second = \"MISO\"\n",
    "\n",
    "fig = px.line(\n",
    "    interchange,\n",
    "    x=interchange.index,\n",
    "    y=[\n",
    "        f\"EBA.{first}-{second}.ID.H\",\n",
    "        f\"EBA.{second}-{first}.ID.H\",\n",
    "        f\"EBA.{first}-ALL.TI.H\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"{first}/{second} interchange\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Interchange\",\n",
    "    legend_title=\"Series\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba = \"PJM\"\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=interchange.index,\n",
    "        y=interchange[f\"EBA.{ba}-ALL.D.H\"] - interchange[f\"EBA.{ba}-ALL.NG.H\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"{ba} demand - generation\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Demand - generation\",\n",
    "    legend_title=\"Series\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign issues across interchange data\n",
    "\n",
    "Most interchanges should be negatively correlated with the interchange coming the other way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a df where columns are interchange data, add best correlation between matching\n",
    "# BAs to interchange_cors dict\n",
    "def interchange_sign(interchange, i_sign: dict = {}, name: str = \"cors\"):\n",
    "    for ba in bas930:\n",
    "        print(ba, end=\"...\")\n",
    "        other_cols = [\n",
    "            c\n",
    "            for c in interchange.columns\n",
    "            if re.split(r\"[-.]\", c)[1] == ba and re.split(r\"[-.]\", c)[2] != \"ALL\"\n",
    "        ]\n",
    "        other_bas = [re.split(r\"[-.]\", c)[2] for c in other_cols]\n",
    "        # print(f\"{ba} connects to {other_bas}\")\n",
    "\n",
    "        out = pd.DataFrame(index=other_bas, columns=range(-12, 12), dtype=float)\n",
    "        for o_ba in out.index:\n",
    "            this_way = f\"EBA.{o_ba}-{ba}.ID.H\"\n",
    "            other_way = f\"EBA.{ba}-{o_ba}.ID.H\"\n",
    "            if other_way not in interchange or this_way not in interchange:\n",
    "                continue\n",
    "            for lag in out.columns:\n",
    "                out.loc[o_ba, lag] = interchange[this_way].corr(\n",
    "                    -1 * interchange[other_way].shift(lag)\n",
    "                )\n",
    "\n",
    "        i_sign[ba] = pd.concat(\n",
    "            [\n",
    "                i_sign.get(ba, pd.DataFrame()),\n",
    "                out.dropna(how=\"all\").max(axis=1).rename(name),\n",
    "            ],\n",
    "            axis=\"columns\",\n",
    "        )\n",
    "\n",
    "    return i_sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_sign = {}\n",
    "for year in [2019, 2020, 2021, 2022, 2023, 2024]:\n",
    "    int_sign = interchange_sign(\n",
    "        interchange[f\"{year}-01-01T00:00\" : f\"{year}-12-30T00:00\"],\n",
    "        int_sign,\n",
    "        name=str(year),\n",
    "    )\n",
    "\n",
    "int_sign = interchange_sign(interchange, int_sign, name=\"all_years\")\n",
    "int_sign = interchange_sign(\n",
    "    interchange[(interchange.index.month >= 4) & (interchange.index.month <= 9)],\n",
    "    int_sign,\n",
    "    name=\"daylight savings\",\n",
    ")\n",
    "int_sign = interchange_sign(\n",
    "    interchange[(interchange.index.month >= 11) | (interchange.index.month <= 2)],\n",
    "    int_sign,\n",
    "    name=\"standard time\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = f\"{outputs_folder('2024')}/interchange_cors_sign.md\"\n",
    "hs = open(file, \"w\")\n",
    "hs.write(\"\\n\\n\")\n",
    "hs.close()\n",
    "\n",
    "for ba, out in int_sign.items():\n",
    "    # add new lines for proper markdown syntax\n",
    "    hs = open(file, \"a\")\n",
    "    hs.write(f\"\\n\\n# {ba}\\n\\n\")\n",
    "    hs.close()\n",
    "\n",
    "    out.to_markdown(file, mode=\"a\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
