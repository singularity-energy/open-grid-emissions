{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate 930 timestamps and interchanges\n",
    "\n",
    "### Generation: \n",
    "\n",
    "Check lagged correlation (-11 to +12 hours) between 930 fossil generation and CEMS fossil generation for each BA over different time bounds: \n",
    "* 2021, 2022\n",
    "* Daylight savings vs. non- daylight savings\n",
    "\n",
    "Run for both raw, shifted, and shifted + basic and rolling filtered 930 data. When shifts are correct, the best correlation in the shifted data should be at lag=0. The best correlation in the non-shifted data can indicate what shift might be appropriate. Manual inspection is required to actually decide whether and how much to lag by. \n",
    "\n",
    "We run with the rolling-filtered data because in some cases large errors can cause anomalous best correlations in the shifted but not filtered 930 data. \n",
    "\n",
    "Note: for correct timestamps, demand data in non-shifted data will be best correlated at lag=-1 because 930 uses end-of-hour timestamps while CEMS uses start-of-hour. \n",
    "\n",
    "### Interchange: \n",
    "\n",
    "Check lagged correlations between pairs of BAs with shared interchange. If timestamps are consistent, the best correlation should be at lag=0. \n",
    "\n",
    "We also check the sign of the best correlation between paired BAs: if they're not negatively correlated, one of the signs may be incorrect. \n",
    "\n",
    "### Edge cases: \n",
    "\n",
    "In some BAs, the shifted data still shows a best correlation at lag != 0, but inspection of the data doesn't show an obvious fix. In these cases, we do nothing and rely on `gridemissions` to make the data consistent. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import timedelta\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# # Tell python where to look for modules.\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../src\")\n",
    "\n",
    "import oge.download_data as download_data\n",
    "import oge.load_data as load_data\n",
    "from oge.column_checks import get_dtypes\n",
    "from oge.filepaths import *\n",
    "import oge.impute_hourly_profiles as impute_hourly_profiles\n",
    "import oge.data_cleaning as data_cleaning\n",
    "import oge.output_data as output_data\n",
    "import oge.emissions as emissions\n",
    "import oge.validation as validation\n",
    "import oge.gross_to_net_generation as gross_to_net_generation\n",
    "import oge.eia930 as eia930"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data before and after shifts\n",
    "# Note: this is very slow! (~30min) because it's pivoting large files.\n",
    "lraw = []\n",
    "lshift = []\n",
    "\n",
    "for year in [2021, 2022]:\n",
    "    print(year)\n",
    "\n",
    "    r = eia930.convert_balance_file_to_gridemissions_format(year)\n",
    "\n",
    "    s = eia930.manual_930_adjust(r)\n",
    "    s = eia930.reformat_chalendar(s)\n",
    "    r = eia930.reformat_chalendar(r)\n",
    "\n",
    "    s = s[s.fuel.isin([\"COL\", \"NG\", \"OIL\"])]\n",
    "    s = s.rename(columns={\"UTC Time at End of Hour\": \"datetime_utc\"})\n",
    "    s = s.groupby([\"datetime_utc\", \"BA\"]).sum()[\"generation\"].reset_index()\n",
    "    s = s[s.datetime_utc.dt.year == year]  # filter for year\n",
    "\n",
    "    # Filter for fossil fuels, sum by BA\n",
    "    r = r[r.fuel.isin([\"COL\", \"NG\", \"OIL\"])]\n",
    "    r = r.rename(columns={\"UTC Time at End of Hour\": \"datetime_utc\"})\n",
    "    r = r.groupby([\"datetime_utc\", \"BA\"]).sum()[\"generation\"].reset_index()\n",
    "    r = r[r.datetime_utc.dt.year == year]  # filter for year\n",
    "    lraw.append(r)\n",
    "    lshift.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.concat(lraw, axis=0)\n",
    "shifted = pd.concat(lshift, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data after shifting and rolling filter\n",
    "all_rolled = []\n",
    "for y in [2021, 2022]:\n",
    "    rolled_930 = pd.read_csv(\n",
    "        f\"{data_folder()}/outputs/{y}/eia930/eia930_rolling.csv\",\n",
    "        index_col=0,\n",
    "        parse_dates=True,\n",
    "    )\n",
    "    rolled_930 = rolled_930[rolled_930.index.year == y]\n",
    "    all_rolled.append(rolled_930)\n",
    "rolled_930 = eia930.reformat_chalendar(pd.concat(all_rolled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove renewables before summing 930\n",
    "rolled_930 = (\n",
    "    rolled_930[rolled_930.fuel.isin([\"COL\", \"NG\", \"OIL\"])]\n",
    "    .groupby([\"datetime_utc\", \"BA\"])\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "# Aggregate by BA during loading to cut down on space\n",
    "cems = pd.DataFrame()\n",
    "for y in [2021, 2022]:\n",
    "    print(f\"loading {y}\")\n",
    "    file = f\"{data_folder()}/outputs/{y}/cems_cleaned_{y}.csv\"\n",
    "    plant_meta = pd.read_csv(\n",
    "        f\"{data_folder()}/outputs/{y}/plant_static_attributes_{y}.csv\"\n",
    "    )\n",
    "    c = pd.read_csv(file, index_col=0, parse_dates=[\"datetime_utc\"])\n",
    "    c = c.merge(\n",
    "        plant_meta[[\"plant_id_eia\", \"plant_primary_fuel\", \"ba_code\"]],\n",
    "        how=\"left\",\n",
    "        left_index=True,\n",
    "        right_on=\"plant_id_eia\",\n",
    "    )\n",
    "    # Exclude solar power for CEMS, since we're just going to look at COL + OIL + NG in the 930 data\n",
    "    c = c[c[\"plant_primary_fuel\"] != \"SUN\"]\n",
    "    c = c[[\"datetime_utc\", \"ba_code\", \"gross_generation_mwh\"]]\n",
    "\n",
    "    print(\"Aggregating\")\n",
    "    if y in [2021, 2022]:\n",
    "        c = c.rename(columns={\"gross_generation_mwh\": \"net_generation_mwh\"})\n",
    "    cems_aggregated = (\n",
    "        c.groupby([\"datetime_utc\", \"ba_code\"]).sum()[\"net_generation_mwh\"].reset_index()\n",
    "    )\n",
    "    cems = pd.concat([cems, cems_aggregated])\n",
    "\n",
    "cems.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2022\n",
    "plant_attributes = pd.read_csv(\n",
    "    f\"{data_folder()}/outputs/{year}/plant_static_attributes_{year}.csv\",\n",
    "    dtype=get_dtypes(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bas = set(raw.BA.unique())\n",
    "bas.intersection_update(set(cems.ba_code.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"shared BAs: {len(bas)} out of {len(raw.BA.unique())} 930 BAs and {len(cems.ba_code.unique())} CEMS BAs.\"\n",
    ")\n",
    "\n",
    "missing_cems = set(raw.BA.unique()).difference(set(cems.ba_code.unique()))\n",
    "missing_930 = set(cems.ba_code.unique()).difference(set(raw.BA.unique()))\n",
    "print(f\"930 BAs missing in CEMS: {missing_cems}\")\n",
    "print(f\"CEMS missing 930: {missing_930}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_cor(cems, df_eia930):\n",
    "    cems = cems.pivot(\n",
    "        columns=\"ba_code\", index=\"datetime_utc\", values=\"net_generation_mwh\"\n",
    "    )\n",
    "    df_eia930 = df_eia930.pivot(columns=\"BA\", index=\"datetime_utc\", values=\"generation\")\n",
    "\n",
    "    bas = set(cems.columns).intersection(set(df_eia930.columns))\n",
    "\n",
    "    correlations = pd.DataFrame(index=list(bas), columns=range(-12, 12), dtype=float)\n",
    "\n",
    "    for ba in correlations.index:\n",
    "        for lag in correlations.columns:\n",
    "            # prepare 930: select BA\n",
    "            # eia = df_eia930[df_eia930.BA==ba][\"generation\"]\n",
    "            # prepare CEMS: select BA\n",
    "            # c = cems[cems.ba_code==ba][\"net_generation_mwh\"]\n",
    "            # calculate\n",
    "            correlations.loc[ba, lag] = cems[ba].corr(df_eia930[ba].shift(lag))\n",
    "\n",
    "    best = correlations.apply(lambda s: s.index[s.argmax()], axis=1).rename(\"best\")\n",
    "\n",
    "    correlations = pd.concat([best, correlations], axis=\"columns\")\n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cems.drop_duplicates(subset=[\"datetime_utc\", \"ba_code\"], inplace=True)\n",
    "# rolled_930.drop_duplicates(subset=[\"datetime_utc\",\"BA\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate best correlations for shifted (no EBA cleaning) data\n",
    "cems_930_cors = pd.concat(\n",
    "    [\n",
    "        find_best_cor(cems, shifted).best.rename(\"all_years\"),\n",
    "        find_best_cor(\n",
    "            cems[cems.datetime_utc.dt.year == 2021],\n",
    "            shifted[shifted.datetime_utc.dt.year == 2021],\n",
    "        ).best.rename(\"2021\"),\n",
    "        find_best_cor(\n",
    "            cems[cems.datetime_utc.dt.year == 2022],\n",
    "            shifted[shifted.datetime_utc.dt.year == 2022],\n",
    "        ).best.rename(\"2022\"),\n",
    "        find_best_cor(\n",
    "            cems[(cems.datetime_utc.dt.month >= 4) & (cems.datetime_utc.dt.month <= 9)],\n",
    "            shifted[\n",
    "                (shifted.datetime_utc.dt.month >= 4)\n",
    "                & (shifted.datetime_utc.dt.month <= 9)\n",
    "            ],\n",
    "        ).best.rename(\"daylight time\"),\n",
    "        find_best_cor(\n",
    "            cems[\n",
    "                (cems.datetime_utc.dt.month >= 11) | (cems.datetime_utc.dt.month <= 2)\n",
    "            ],\n",
    "            shifted[\n",
    "                (shifted.datetime_utc.dt.month >= 11)\n",
    "                | (shifted.datetime_utc.dt.month <= 2)\n",
    "            ],\n",
    "        ).best.rename(\"standard time\"),\n",
    "    ],\n",
    "    axis=\"columns\",\n",
    ")\n",
    "\n",
    "cems_930_cors.to_csv(f\"{data_folder()}/outputs/2022/cems_SHIFTEDeia930_cor_lags.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate best correlations for raw data\n",
    "cems_930_cors = pd.concat(\n",
    "    [\n",
    "        find_best_cor(cems, raw).best.rename(\"all_years\"),\n",
    "        find_best_cor(\n",
    "            cems[cems.datetime_utc.dt.year == 2021],\n",
    "            raw[raw.datetime_utc.dt.year == 2021],\n",
    "        ).best.rename(\"2021\"),\n",
    "        find_best_cor(\n",
    "            cems[cems.datetime_utc.dt.year == 2022],\n",
    "            raw[raw.datetime_utc.dt.year == 2022],\n",
    "        ).best.rename(\"2022\"),\n",
    "        find_best_cor(\n",
    "            cems[(cems.datetime_utc.dt.month >= 4) & (cems.datetime_utc.dt.month <= 9)],\n",
    "            raw[(raw.datetime_utc.dt.month >= 4) & (raw.datetime_utc.dt.month <= 9)],\n",
    "        ).best.rename(\"daylight time\"),\n",
    "        find_best_cor(\n",
    "            cems[\n",
    "                (cems.datetime_utc.dt.month >= 11) | (cems.datetime_utc.dt.month <= 2)\n",
    "            ],\n",
    "            raw[(raw.datetime_utc.dt.month >= 11) | (raw.datetime_utc.dt.month <= 2)],\n",
    "        ).best.rename(\"standard time\"),\n",
    "    ],\n",
    "    axis=\"columns\",\n",
    ")\n",
    "\n",
    "cems_930_cors.to_csv(f\"{data_folder()}/outputs/2022/cems_RAWeia930_cor_lags.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate correlations using different subsets of 930 data\n",
    "\n",
    "cems_930_cors = pd.concat(\n",
    "    [\n",
    "        find_best_cor(cems, rolled_930).best.rename(\"all_years\"),\n",
    "        find_best_cor(\n",
    "            cems[cems.datetime_utc.dt.year == 2021],\n",
    "            rolled_930[rolled_930.datetime_utc.dt.year == 2021],\n",
    "        ).best.rename(\"2021\"),\n",
    "        find_best_cor(\n",
    "            cems[cems.datetime_utc.dt.year == 2022],\n",
    "            rolled_930[rolled_930.datetime_utc.dt.year == 2022],\n",
    "        ).best.rename(\"2022\"),\n",
    "        find_best_cor(\n",
    "            cems[(cems.datetime_utc.dt.month >= 4) & (cems.datetime_utc.dt.month <= 9)],\n",
    "            rolled_930[\n",
    "                (rolled_930.datetime_utc.dt.month >= 4)\n",
    "                & (rolled_930.datetime_utc.dt.month <= 9)\n",
    "            ],\n",
    "        ).best.rename(\"daylight time\"),\n",
    "        find_best_cor(\n",
    "            cems[\n",
    "                (cems.datetime_utc.dt.month >= 11) | (cems.datetime_utc.dt.month <= 2)\n",
    "            ],\n",
    "            rolled_930[\n",
    "                (rolled_930.datetime_utc.dt.month >= 11)\n",
    "                | (rolled_930.datetime_utc.dt.month <= 2)\n",
    "            ],\n",
    "        ).best.rename(\"standard time\"),\n",
    "    ],\n",
    "    axis=\"columns\",\n",
    ")\n",
    "\n",
    "cems_930_cors.to_csv(f\"{data_folder()}/outputs/2022/cems_RAWeia930_cor_lags.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a BA for manual inspection\n",
    "ba = \"TEPC\"\n",
    "\n",
    "to_plot_930 = shifted[shifted.BA == ba].groupby(\"datetime_utc\").sum()\n",
    "\n",
    "print(f\"correlations for {ba}\")\n",
    "print(cems_930_cors.loc[ba])\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=cems[cems.ba_code == ba].datetime_utc,\n",
    "        y=cems[cems.ba_code == ba].net_generation_mwh,\n",
    "        name=\"CEMS\",\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=to_plot_930.index,\n",
    "        y=to_plot_930.generation,\n",
    "        name=\"EIA 930 (after adjustment and rolling cleaning)\",\n",
    "    )\n",
    ")\n",
    "fig.update_layout(title=ba, xaxis_title=\"Date\", yaxis_title=\"Generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interchanges = []\n",
    "for year in [2021, 2022]:\n",
    "    interchange = pd.read_csv(\n",
    "        f\"{data_folder()}/outputs/{year}/eia930/eia930_raw.csv\",\n",
    "        index_col=0,\n",
    "        parse_dates=True,\n",
    "    )\n",
    "    interchange = interchange[\n",
    "        interchange.index.year == year\n",
    "    ]  # limit to after gen was reported by fuel type\n",
    "    interchanges.append(interchange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interchange = pd.concat(interchanges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bas930 = {re.split(r\"[-.]\", c)[1] for c in interchange.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a df where columns are interchange data, add best correlation between matching BAs to interchange_cors dict\n",
    "# optionally, write markdown to {file}.md and csvs at {file}_{ba}.csv\n",
    "def interchange_cor(\n",
    "    interchange, interchange_cors: dict = {}, file=\"\", name: str = \"cors\"\n",
    "):\n",
    "    # Delete file\n",
    "    if file != \"\":\n",
    "        hs = open(file + \".md\", \"w\")\n",
    "        hs.write(\"\\n\\n\")\n",
    "        hs.close()\n",
    "\n",
    "    for ba in bas930:\n",
    "        print(ba, end=\"...\")\n",
    "        other_cols = [\n",
    "            c\n",
    "            for c in interchange.columns\n",
    "            if re.split(r\"[-.]\", c)[1] == ba and re.split(r\"[-.]\", c)[2] != \"ALL\"\n",
    "        ]\n",
    "        other_bas = [re.split(r\"[-.]\", c)[2] for c in other_cols]\n",
    "        # print(f\"{ba} connects to {other_bas}\")\n",
    "\n",
    "        out = pd.DataFrame(index=other_bas, columns=range(-12, 12), dtype=float)\n",
    "        for o_ba in out.index:\n",
    "            this_way = f\"EBA.{o_ba}-{ba}.ID.H\"\n",
    "            other_way = f\"EBA.{ba}-{o_ba}.ID.H\"\n",
    "            if (\n",
    "                other_way not in interchange.columns\n",
    "                or this_way not in interchange.columns\n",
    "            ):\n",
    "                continue\n",
    "            for lag in out.columns:\n",
    "                out.loc[o_ba, lag] = abs(\n",
    "                    interchange[this_way].corr(-1 * interchange[other_way].shift(lag))\n",
    "                )\n",
    "\n",
    "        # where is correlation the best?\n",
    "        out = pd.concat(\n",
    "            [out, out.apply(lambda s: s.index[s.argmax()], axis=1).rename(\"best\")],\n",
    "            axis=\"columns\",\n",
    "        )\n",
    "\n",
    "        if file != \"\":\n",
    "            # add new lines for proper markdown syntax\n",
    "            hs = open(file + \".md\", \"a\")\n",
    "            hs.write(f\"\\n\\n# {ba}\\n\\n\")\n",
    "            hs.close()\n",
    "\n",
    "            out.to_markdown(file + \".md\", mode=\"a\")\n",
    "\n",
    "            out.to_csv(f\"{file}_{ba}\" + \".csv\")\n",
    "\n",
    "        interchange_cors[ba] = pd.concat(\n",
    "            [interchange_cors.get(ba, pd.DataFrame()), out.best.rename(name)],\n",
    "            axis=\"columns\",\n",
    "        )\n",
    "\n",
    "    return interchange_cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cors = interchange_cor(interchange, interchange_cors={}, name=\"all_years\")\n",
    "int_cors = interchange_cor(\n",
    "    interchange[\"2021-01-01T00:00\":\"2021-12-30T00:00\"], int_cors, name=\"2021\"\n",
    ")\n",
    "int_cors = interchange_cor(\n",
    "    interchange[\"2022-01-01T00:00\":\"2022-12-30T00:00\"], int_cors, name=\"2022\"\n",
    ")\n",
    "int_cors = interchange_cor(\n",
    "    interchange[(interchange.index.month >= 4) & (interchange.index.month <= 9)],\n",
    "    int_cors,\n",
    "    name=\"daylight savings\",\n",
    ")\n",
    "int_cors = interchange_cor(\n",
    "    interchange[(interchange.index.month >= 11) | (interchange.index.month <= 2)],\n",
    "    int_cors,\n",
    "    name=\"standard time\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect interchange correlations\n",
    "int_cors[\"PJM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to md file because that's an easy way to manually scan through BAs and look for anomalies\n",
    "file = f\"{data_folder()}/outputs/2022/interchange_corr_summary_adjusted.md\"\n",
    "hs = open(file, \"w\")\n",
    "hs.write(\"\\n\\n\")\n",
    "hs.close()\n",
    "\n",
    "for ba, out in int_cors.items():\n",
    "    # add new lines for proper markdown syntax\n",
    "    hs = open(file, \"a\")\n",
    "    hs.write(f\"\\n\\n# {ba}\\n\\n\")\n",
    "    hs.close()\n",
    "\n",
    "    out.to_markdown(file, mode=\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot interchange for BA of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba1 = \"IID\"\n",
    "ba2 = \"CISO\"\n",
    "\n",
    "fig = px.line(interchange[f\"EBA.{ba1}-{ba2}.ID.H\"])\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=interchange.index,\n",
    "        y=interchange[f\"EBA.{ba2}-{ba1}.ID.H\"],\n",
    "        name=f\"EBA.{ba2}-{ba1}.ID.H\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba = \"PJM\"\n",
    "\n",
    "# find cols of mappings in both directions\n",
    "other_cols = [\n",
    "    c\n",
    "    for c in interchange.columns\n",
    "    if re.split(r\"[-.]\", c)[1] == ba and re.split(r\"[-.]\", c)[2] != \"ALL\"\n",
    "]\n",
    "other_bas = [re.split(r\"[-.]\", c)[2] for c in other_cols]\n",
    "\n",
    "these_cols = [f\"EBA.{o_ba}-{ba}.ID.H\" for o_ba in other_bas]\n",
    "\n",
    "# make long version with just cols of interest, adding BA column and to/from column\n",
    "toplot = pd.DataFrame()\n",
    "for i in range(len(other_bas)):\n",
    "    to_add = (interchange[other_cols[i]]).rename(\"interchange\").to_frame()\n",
    "    to_add[\"source\"] = ba\n",
    "    to_add[\"BA\"] = other_bas[i]\n",
    "\n",
    "    to_add_2 = (interchange[these_cols[i]] * (-1)).rename(\"interchange\").to_frame()\n",
    "    to_add_2[\"source\"] = \"other BA\"\n",
    "    to_add_2[\"BA\"] = other_bas[i]\n",
    "\n",
    "    toplot = pd.concat([toplot, to_add, to_add_2], axis=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    toplot,\n",
    "    x=toplot.index,\n",
    "    y=\"interchange\",\n",
    "    facet_col=\"BA\",\n",
    "    facet_col_wrap=2,\n",
    "    color=\"source\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=f\"Interchange from {ba}\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Interchange\",\n",
    "    legend_title=\"Source for<br>interchange data\",\n",
    ")\n",
    "fig.for_each_annotation(lambda a: a.update(text=\"Other \" + a.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = \"PJM\"\n",
    "second = \"MISO\"\n",
    "\n",
    "fig = px.line(\n",
    "    interchange,\n",
    "    x=interchange.index,\n",
    "    y=[\n",
    "        f\"EBA.{first}-{second}.ID.H\",\n",
    "        f\"EBA.{second}-{first}.ID.H\",\n",
    "        f\"EBA.{first}-ALL.TI.H\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"{first}/{second} interchange\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Interchange\",\n",
    "    legend_title=\"Series\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba = \"PJM\"\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=interchange.index,\n",
    "        y=interchange[f\"EBA.{ba}-ALL.D.H\"] - interchange[f\"EBA.{ba}-ALL.NG.H\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"{ba} demand - generation\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Demand - generation\",\n",
    "    legend_title=\"Series\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign issues across interchange data\n",
    "\n",
    "Most interchanges should be negatively correlated with the interchange coming the other way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a df where columns are interchange data, add best correlation between matching BAs to interchange_cors dict\n",
    "# optionally, write markdown to {file}.md and csvs at {file}_{ba}.csv\n",
    "def interchange_sign(interchange, i_sign: dict = {}, file=\"\", name: str = \"cors\"):\n",
    "    for ba in bas930:\n",
    "        print(ba, end=\"...\")\n",
    "        other_cols = [\n",
    "            c\n",
    "            for c in interchange.columns\n",
    "            if re.split(r\"[-.]\", c)[1] == ba and re.split(r\"[-.]\", c)[2] != \"ALL\"\n",
    "        ]\n",
    "        other_bas = [re.split(r\"[-.]\", c)[2] for c in other_cols]\n",
    "        # print(f\"{ba} connects to {other_bas}\")\n",
    "\n",
    "        out = pd.DataFrame(index=other_bas, columns=range(-12, 12), dtype=float)\n",
    "        for o_ba in out.index:\n",
    "            this_way = f\"EBA.{o_ba}-{ba}.ID.H\"\n",
    "            other_way = f\"EBA.{ba}-{o_ba}.ID.H\"\n",
    "            if other_way not in interchange or this_way not in interchange:\n",
    "                continue\n",
    "            for lag in out.columns:\n",
    "                out.loc[o_ba, lag] = interchange[this_way].corr(\n",
    "                    -1 * interchange[other_way].shift(lag)\n",
    "                )\n",
    "\n",
    "        # where is correlation the best?\n",
    "        out = out.apply(lambda s: s.iloc[abs(s).argmax()], axis=1)\n",
    "\n",
    "        i_sign[ba] = pd.concat(\n",
    "            [i_sign.get(ba, pd.DataFrame()), out.rename(name)], axis=\"columns\"\n",
    "        )\n",
    "\n",
    "    return i_sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_sign = interchange_sign(interchange, {}, name=\"all_years\")\n",
    "int_sign = interchange_sign(\n",
    "    interchange[\"2021-01-01T00:00\":\"2021-12-30T00:00\"], int_sign, name=\"2021\"\n",
    ")\n",
    "int_sign = interchange_sign(\n",
    "    interchange[\"2022-01-01T00:00\":\"2022-12-30T00:00\"], int_sign, name=\"2022\"\n",
    ")\n",
    "int_sign = interchange_sign(\n",
    "    interchange[(interchange.index.month >= 4) & (interchange.index.month <= 9)],\n",
    "    int_sign,\n",
    "    name=\"daylight savings\",\n",
    ")\n",
    "int_sign = interchange_sign(\n",
    "    interchange[(interchange.index.month >= 11) | (interchange.index.month <= 2)],\n",
    "    int_sign,\n",
    "    name=\"standard time\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = f\"{outputs_folder('2022')}/interchange_cors_sign.md\"\n",
    "hs = open(file, \"w\")\n",
    "hs.write(\"\\n\\n\")\n",
    "hs.close()\n",
    "\n",
    "for ba, out in int_sign.items():\n",
    "    # add new lines for proper markdown syntax\n",
    "    hs = open(file, \"a\")\n",
    "    hs.write(f\"\\n\\n# {ba}\\n\\n\")\n",
    "    hs.close()\n",
    "\n",
    "    out.to_markdown(file, mode=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('open_grid_emissions')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3035f9e4886fbbd36e5472dea39a16278dd2b875736b64746b1db5f69a1d6c82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
